{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "def PDF(url):\n",
    "    return HTML('<iframe src=%s width=700 height=350></iframe>' % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On imagine qu’on veut présenter une feuille à une caméra, sur laquelle vous on écrit un chiffre. On aimerait que l’algorithme derrière la caméra soit capable de déterminer, avec précision, le chiffre qu’on a écrit.  \n",
    "\n",
    "Ce problème peut paraitre simple à traiter, mais il en va différemment pour un ordinateur. Nous allons donc tenter, non pas de lui faire résoudre le problème « quel est ce chiffre ? », mais plutôt de lui faire résoudre « Comment apprendre à reconnaitre un chiffre ? ».  \n",
    "\n",
    "Prenons une image de 28×28 pixels, à savoir le format standard pour le challenge MNIST (Modified National Institute of Standards and Technology) de reconnaissance des chiffres. Cette image est fournie en noir et blanc, avec comme seule inscription un chiffre.\n",
    "\n",
    "Pour représenter cette image, nous allons utiliser un vecteur binaire de 784 variables (28×28). La ième valeur du vecteur prendra la valeur 0 ou 1 suivant si le ième pixel dans l’image est blanc ou noir.  \n",
    "\n",
    "Un algorithme de Deep Learning va commencer par subdiviser ce problème complexe, en sous problèmes plus simple à résoudre. Pour cela, il va prendre des sous parties de l’image, par exemple, des groupements de pixels de 5×5, 4×4, ou 3×3, et opérer une fonction afin d’extraire des motifs dans cette sous-partie (on parle d’opération de convolution).  \n",
    "\n",
    "Ces fonctions permettent de faire ressortir certaines caractéristiques des images.\n",
    "Ainsi, différentes transformations sont opérées sur différentes parties de l’image d’entrée, retravaillées puis combinées à la couche suivante, etc. permettant ainsi de générer des cartographies des pixels importants dans l’image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données\n",
    "Les données sont séparées en 2 sous-parties : \n",
    "- 60 000 images pour la base train\n",
    "- 10 000 images pour la base test  \n",
    "\n",
    "On note que 250 personnes différentes, et de background variés constituent la base train. Les 250 personnes de la base test sont encore des personnes encore une fois différentes mais appartenant aux mêmes catégories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=./docs/neuralnetworksanddeeplearning_com_chap1_html.pdf width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF(\"./docs/neuralnetworksanddeeplearning_com_chap1_html.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=./docs/neuralnetworksanddeeplearning_com_chap2_html.pdf width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF(\"./docs/neuralnetworksanddeeplearning_com_chap2_html.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Précédemment nous avons évoqué comment les réseaux de neurones pouvaient apprendre les poids et les biais en utilisant l’algorithme de la descente du gradient. Pour pouvoir calculer la fonction du coût nous avons recours à un algorithme rapide sous le nom de retro propagation.  \n",
    "\n",
    "Quand on parle de perceptron, ou perceptron multicouche, l’accent est mis sur l’algorithme d’apprentissage sur ces deux modèles, en particulier sur l’algorithme de rétro propagation (back propagation en anglais). Cet algorithme est, en effet, le premier algorithme d'apprentissage convaincant dans un modèle suffisamment puissant et cet algorithme à de nombreuses applications.\n",
    "\n",
    "## Objectif\n",
    "\n",
    "Cette partie du réseau de neurones est souvent traitée comme boite noire, dont certains sont prêts à omettre les détails. Au cœur de la retro propagation et la dérivée ∂C/∂w de la fonction du coût C par rapport au poids w ou au biais b dans le réseau.  \n",
    "\n",
    "Cette fonction du coût mesure l’écart entre les prédictions des neurones de sorties et les valeurs cibles spécifiés dans l’ensemble d’entraînement. On cherche à minimiser la somme ou la moyenne des erreurs sur toutes les configurations de l’ensemble d’entrainement.  \n",
    "L’expression détermine à quelle vitesse le coût peut varier quand on change les poids et les biais. Donc cet algorithme nous permet de juger comment les poids et biais d’un réseau peuvent affecter don comportement.\n",
    "\n",
    "## Faiblesses \n",
    "\n",
    "1.\tL’expérience montre que le temps d’entraînement d’un réseau de neurone croît rapidement lorsque le nombre de couches augmente. C’est d’ailleurs l’une des raisons pour lesquelles à les réseaux de neurones ont été remisés au profit d’autres algorithmes non-linéaires moins gourmand en ressources comme les SVM.  \n",
    "\n",
    "2.\tLes réseaux de neurones n’échappent pas au problème central du Machine Learning : l’overfitting.\n",
    "\n",
    "## Fonctionnement de l’algorithme\n",
    "\n",
    "D’une manière générale, le but est de calculer les dérivés partiels ∂C/∂w et ∂C/∂b de la fonction du coût C par rapport à n’importe quel poids w ou biais b dans le réseau.\n",
    "- Entrée :\n",
    "- Feedforward\n",
    "- Erreur de sortie\n",
    "- Retro propagation de l’erreur\n",
    "- Sortie\n",
    "One remarque que le mouvement à reculons est une conséquence du fait que le coût est une fonction de sorties du réseau. \n",
    "\n",
    "Dans la class Network nous avons deux méthodes :\n",
    "- update_mini_batch : met à jour les poids et biais en calculant le gradient pour le mini_batch courrant. Ce dernier est une liste de tuples avec un taux d’apprentissage\n",
    "\n",
    "- backprop : calcule les dérivées partielles décrites précédemment; Cette méthode retourne un tuple qui représente le gradient pour la fonction du coût. A noter que nabla_b et nabla_w sont des couche-par-couche listes de matrices numpy.\n",
    "\n",
    "Les étapes décrites précédemment se succèdent également et logiquement dans l’algorithme. \n",
    "\n",
    "L’idée novatrice de l’algorithme de rétro propagation est qu’elle nous permet de calculer simultanément toutes les dérivés partiels en utilisant un pass à travers le réseau suivis par une pass en arrière.  \n",
    "\n",
    "La question qu’on se pose souvent : Qu’est-ce que l’algorithme fait réellement ? On peut dire que l’erreur est retro-propagée  de la sortie. Mais peut-on être sûr et éventuellement déduire ce qu’ils se passent quand nous faisons les multiplications de vecteurs et matrices ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=./docs/neuralnetworksanddeeplearning_com_chap3_html.pdf width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF(\"./docs/neuralnetworksanddeeplearning_com_chap3_html.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=./docs/neuralnetworksanddeeplearning_com_chap6_html.pdf width=700 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF(\"./docs/neuralnetworksanddeeplearning_com_chap6_html.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude à partir d'une librairie python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ici, pour aller plus loin utiliser une librairie Python permettant d'implémenter des réseaux de perceptrons. Il est assez facile de mettre en place un réseau de neuronnes avec la bibliothèque Scikit-Learn. Il est implémenté de nombreux algorithmes utilisés pour le machine learning ainsi que pour le nettoyage des données ou l'extraction de variables. \n",
    "Il existe notamment une implémentation des réseaux de perceptrons MLP([Multi-Layer Perceptron](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)). Deux implémentations ont été mise en place. Une pour la classification et l'autre pour la régression MLPClassifier et MLPRegressor. Dans notre cas nous allons utiliser l'implémentation permettant de classifier car nous voulons déterminer le chiffre représenté par une écriture manuscrite.\n",
    "\n",
    "On peut très rapidement rappeler les différents avantages et inconvéniant de ces réseaux de plusieurs couches de perceptrons. \n",
    "\n",
    "#### The advantages of Multi-layer Perceptron are:\n",
    "- Capability to learn non-linear models.\n",
    "- Capability to learn models in real-time (on-line learning) using partial_fit.\n",
    "\n",
    "#### The disadvantages of Multi-layer Perceptron (MLP) include:\n",
    "    \n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to feature scaling.\n",
    "\n",
    "On représentera cette écriture par une image qui sera elle même représentée par une matrice de pixels. Les images sont toutes de la même taille 28x28 pixels, soit 784 pixels. Nous aurons donc 784 variables représentant la couleur du pixel 1 noir, 0 blanc. \n",
    "\n",
    "Nous avons utilisé les données proposées par Kaggle dans le cadre de leur [compétition](https://www.kaggle.com/c/digit-recognizer) . Ils mettent à disposition deux fichiers d'entrainement et de test pour réaliser de la classification sur les écritures manuscrites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etudes des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MLPClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-80db8e28377d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MLPClassifier'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc 42 000 exemples de chiffres et 784 pixels + 1 colonne représentant le label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps on peut afficher quelques chiffres pour comprendre les données..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_matrix_image_random(df):\n",
    "    return np.array(df.sample(1).drop([\"label\"], axis=1)).reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "gs = gridspec.GridSpec(3, 3)\n",
    "fig.set_size_inches(10,10)\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax1.imshow(get_matrix_image_random(train), cmap='gray')\n",
    "ax2 = fig.add_subplot(gs[0,-1])\n",
    "ax2.imshow(get_matrix_image_random(train), cmap='gray')\n",
    "ax3 = fig.add_subplot(gs[0,-2])\n",
    "ax3.imshow(get_matrix_image_random(train), cmap='gray')\n",
    "ax4 = fig.add_subplot(gs[-1,0])\n",
    "ax4.imshow(get_matrix_image_random(train), cmap='gray')\n",
    "ax5 = fig.add_subplot(gs[-1,-1])\n",
    "ax5.imshow(get_matrix_image_random(train), cmap='gray')\n",
    "ax6 = fig.add_subplot(gs[-1,-2])\n",
    "ax6.imshow(get_matrix_image_random(train), cmap='gray')\n",
    "ax7 = fig.add_subplot(gs[-2,0])\n",
    "ax7.imshow(get_matrix_image_random(train), cmap='gray')\n",
    "ax8 = fig.add_subplot(gs[-2,-1])\n",
    "ax8.imshow(get_matrix_image_random(train), cmap='gray')\n",
    "ax9 = fig.add_subplot(gs[-2,-2])\n",
    "ax9.imshow(get_matrix_image_random(train), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite commencer à créer un réseau de neuronnes très simple. Il comportera 5 couches de 2 perceptrons. On peut très facilement modifier les paramètres de ces réseaux de neuronnes. Il faut quand même faire attention  à la complexité des algorithmes. En effet, l'implémentation proposée par scikit learn utilise uniquement de la mémoire RAM ce qui peut être très vite assez limité. D'autres implémentations plus performantes utilsent la GPU pour effectuer des prédictions sur des ensembles de données plus importants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(5, 2),random_state=1)\n",
    "\n",
    "clf.fit(X, y)  \n",
    "clf.predict([[2., 2.], [-1., -2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = train.drop(\"label\" , axis = 1)\n",
    "y = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Counter(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances sur l'ensemble d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(clf.predict(X_train), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import imshow, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_plot = X_test.copy()\n",
    "\n",
    "X_plot[\"label\"] = y_test\n",
    "X_plot[\"predict\"] = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_error.shape[0]/X_plot.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X_error.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut essayer d'afficher quels sont les chiffres les mieux ou les moins bien classifié avec un histogram représentant le taux de bonne classification. Comme les différentes classes sont assez équitablement réparties on peut se permettre de ne pas normaliser les données pour conserver une compréhension du nombre de mauvaise classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_error = X_plot[X_plot.label != X_plot.predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16,6)\n",
    "ax.hist(X_error.label)\n",
    "ax.set_xlim(0, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir ici que les 0 et 1 et les 6 sont très bien classifiés dans l'ensemble. Il y a plus de problèmes sur les autres classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_elemtn = X_error.shape[0]\n",
    "n_elemtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite essayer de comprendre quels sont les chiffres prédits par rapport à l'image. Il est plus ou moins évident de comprendre \"pourquoi\" il a pu se tromper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int_to_plot = randint(0,n_elemtn)\n",
    "imshow(np.array(X_error.drop(['label', \"predict\"], axis=1).iloc[int_to_plot]).reshape(28,28), cmap='gray')\n",
    "print(X_error.iloc[int_to_plot][\"predict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int_to_plot = randint(0,n_elemtn)\n",
    "imshow(np.array(X_error.drop(['label', \"predict\"], axis=1).iloc[int_to_plot]).reshape(28,28), cmap='gray')\n",
    "print(X_error.iloc[int_to_plot][\"predict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int_to_plot = randint(0,n_elemtn)\n",
    "imshow(np.array(X_error.drop(['label', \"predict\"], axis=1).iloc[int_to_plot]).reshape(28,28), cmap='gray')\n",
    "print(X_error.iloc[int_to_plot][\"predict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int_to_plot = randint(0,n_elemtn)\n",
    "imshow(np.array(X_error.drop(['label', \"predict\"], axis=1).iloc[int_to_plot]).reshape(28,28), cmap='gray')\n",
    "print(X_error.iloc[int_to_plot][\"predict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_error[\"val_combination\"] = X_error.label.apply(str) + \",\" +  X_error.predict.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix = np.zeros((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = X_error[\"val_combination\"].value_counts()\n",
    "for ind in values.index:\n",
    "    i = int(ind.split(\",\")[0])\n",
    "    j = int(ind.split(\",\")[1])\n",
    "    matrix[i][j] = values[ind]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.imshow(matrix, cmap='hot', interpolation='nearest')\n",
    "heatmap = plt.pcolor(matrix, cmap=\"hot\")\n",
    "plt.colorbar(heatmap)\n",
    "plt.ylabel(\"Label\")\n",
    "plt.ylabel(\"Prédiction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici que les 3 sont souvent associés a des 5 et des 2 et que les 9 sont souvent associés a des 4 et des 8. Nous avons aussi les "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_error[\"val_combination\"].value_counts(dropna=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici les plus mauvaises classifications triées par ordre décroissant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test en mélangeant les labels permettant l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_shuffled = y_train.values.copy()\n",
    "np.random.shuffle(y_train_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performances sur l'ensemble d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(clf.predict(X_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vulgarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le réseau de neurones traditionnel nous permet de traduire 10,000 chiffres manuscrits en chiffres reconnus. Au début, le réseau de neurones traduit les chiffes en pixel avec une valeur « 0 » pour les pixels noirs et « 1 » pour les pixels blancs. Chaque pixel dans une image représentera une variable.  \n",
    "\n",
    "Sur un test de 1000 images manuscrites, le modèle a identifié 922 parmi elle, donc avec une précision de 92.2%. Ci-dessous une table de contingence pour observer les résultats :  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./img/table.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette table de contingence montre la performance du modèle ANN (Artificial Neural Network). Par exemple, la première ligne identifie le chiffre 0 correctement presque tout le temps (84/85 avec une confusion avec le chiffre 6).  \n",
    "\n",
    "D’après la table, on remarque que les chiffres « 0 » et « 1 » sont identifiés correctement assez fréquemment. Par contre, le chiffre « 5 » est moins évident à identifier. En plus, la table de contingence nous montre que le chiffre 2 est confondu avec les chiffes 7 et 8 à peu près 8% du temps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='./img/digit1.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alors que les images peuvent sembler évidentes comme le chiffre « 2 » pour un humain, le réseau de neurones a du mal à reconnaître des aspects d’images comme la « queue » du chiffre 2.\n",
    "Les chiffres « 3 » et « 5 » sont confondus 10% du temps également\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='./img/digit2.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les neurones qui ont inspiré le réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre cerveau est un grand réseau de neurones liés, qui permettent la transmission de l’information d’un point A à un point B. Quand différentes informations sont énvoyées du point A au point B, différents neurones sont activés et donc le cerveau utilise différentes routes pour passer de A à B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='./img/neuron.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dns chaque neurone, les dendrites reçoivent les signaux émis par d’autres neurones. Su un neurone reçoit un niveau élevé de signaux d’un intervalle de temps, le neurone envoie une impulsion électrique vers les terminaux. Ces signaux sont donc reçus par d’autres neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='./img/nn1.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tQuand le nœud d’entrée prend une image, il active uniquement une série de neurones, qui lance une chaîne de réaction pour créer un chemin unique pour le nœud de sortie. Dans le premier scénario, les neurones A, B, et D sont activés à la première couche.\n",
    "\n",
    "2.\tLes neurones activés envoient des signaux à chaque neurone dans la prochaine couche. Ceci affecte directement les neurones de la prochaine couche. Dans le premier scénario, le neurone A envoie un signal à E et G, le neurone B à E, et le neurone D à F et G.\n",
    "\n",
    "3.\tDans la prochaine couche, chaque neurone établit des règles de combinaisons de signaux reçus qui activent un neurone. Dans le premier scénario, le neurone E est activé par les signaux A et B. Par contre, pour les neurones F et G, leurs règles leur dit qu’ils n’ont pas reçu les bons signaux à être activés, et donc ils restent gris.\n",
    "\n",
    "4.\tLes étapes 2 et 3 sont répétés pour toutes les autres couches, jusqu’à ce qu’on se retrouve uniquement avec le nœud de sortie.\n",
    "\n",
    "5.\tLe nœud de sortie déduit le bon chiffre basé sur les signaux de la couche précédente. Chaque combinaison de neurones activés dans cette couche mène à une solution, même si la combinaison est différente. Dans les scénarios 1 et 2, deux images différentes sont fournis, et donc deux chemins différents sont suivi, et le bon chiffre « 6 » est quand même reconnu dans les 2 cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au début, il faut définir le nombre de couches ainsi que le nombre de neurones pour chaque couche de notre réseau. Il n’y a pas forcément de limite, mais 3 couches semble un bon départ vu que c’est proportionnel au nombre de variables. On utilise 3 couches avec 500 neurones pour chacune. On note deux facteurs clé :\n",
    "- Une métrique pour évaluer la précision du modèle -> MSE (qu’on va chercher à minimiser)\n",
    "- Des règles pour définir si un neurone est activé ou non.\n",
    "    - Les poids des signaux rentrants\n",
    "    - Le minimum du signal reçu dont le réseau a besoin pour l’activation\n",
    "\n",
    "Dans l’exemple suivant, les règles pour le neurone G\n",
    "- Le poids pour les signaux A et B est nul (aucune connexion)\n",
    "- C, D, et E ont des poids de 1, 2, et -1 respectivement. \n",
    "\n",
    "La valeur minimale pour G est 2, donc G est activé si :\n",
    "- D est activé et E ne l’est pas OU\n",
    "- C et D sont activés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='./img/nn21.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='./img/nn22.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limites/raisons qui nous poussent vers des réseaux  convolutionnels et le Deep Learning\n",
    "\n",
    "Entraîner un réseau de neurone consomme beaucoup de temps et de CPU par rapport à d’autres types de modèles comme les forêts aléatoires, par exemple. En plus, la performance n’est pas forcément supérieure.\n",
    "Le réseau de neurone décrit au-dessus est la base de modèle plus avancés comme les DNN (Deep Neural Network).\n",
    "Ce modèle n’arrive pas à reconnaître les images si celles si sont légèrement déformés, ou subissent des translations. On a donc une amélioration de ce réseau, avec les CNN (Convolutional Neural Networks) qui résout ce type de problèmes en analysant différents régions d’une image.\n",
    "\n",
    "## Important\n",
    "\n",
    "A noter que durant cette démonstration, la méthode d’activation utilisé s’apelle la fonction d’activation. Elle renvoie 0 si l’entrée est faible et renvoie un résultat positif si l’entrée est suffisamment puissante. Dans la démonstration Python qui va suivre, la fonction qui va activer sera une fonction sigmoïde. Le nœud de sortie sera aussi influencé une fonction, et une généralisation de la fonction logistique. C’est d’ailleurs pourquoi on utilise la différenciation pour trouver les poids corrects grâce à la descente du gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique d’améliorations\n",
    "### Distorsion\n",
    "Un réseau de neurones apprend à reconnaitre les chiffres manuscrits quand on en plus dans notre base train. Ainsi, une large training dataset avec des images labélisées est important. Une manière pour augmenter la taille est de créer plus de données. En appliquant différents distorsions aux images existantes, chaque image altérée pourra être traitée comme un nouvel exemple de training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='./img/digit3.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulation comportement humain.  \n",
    "\n",
    "On peut appliquer des rotations pour simuler l’angle avec lequel les personnes écrivent. On peut également étirer et serrer à un certain point pour simuler des oscillations non-contrôlées de muscles\n",
    "\n",
    "### Technique mini-batch gradient de décente  \n",
    "On utilise la décente de gradient pour influencer l’activation des neurones est améliorer la précision globale. Le réseau de neurones balaye chaque exemple d’entraînement pour déterminer la meilleure disposition des règles. Même si une dataset plus grand améliore la prédiction, elle augmente aussi le temps pour traiter les données. Une solution plus efficace existe, prendre un échantillon à chaque foi pour approximer le meilleure changement de règle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html \"./Hand Written Digits Recognition.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
