{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Dans cette partie nous allons annalyser en details les variables importante du projet\n",
    "ainsi que les fonctions utiliser et voir dans quels but elles sont utiliser.\n",
    "A la fin de cette partie, nous avons effectuer plusieurs test en changeant \n",
    "certains parametres et nous avons essayer d'apporter des explications \n",
    "aux resultats obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = network[x1,x2,x3]\n",
    "\n",
    "\"\"\"la liste sizes contient le nombre de neuronnes dans chaque couche.\n",
    "L'objet net permet de fixer le nombre de neuronnes dans les couches\n",
    "\n",
    "Les biais et les poids sont initialisés de façon aléatoire.\n",
    "\n",
    "Pour la 1 ere couche le biais n'est pas pris en compte , le biais est utilisé seulement pour les \n",
    "couches superieurs. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\"\"\"\n",
    "Pour mettre en place notre réseau de neuronnes, nous definissons un model sigmoidals\n",
    "car il s'adapte bien au algorithmes d'apprentissage utilisant une rétro-propagation du gradien\n",
    "car leur fonction d'activation est derivable.\n",
    "\n",
    "Une fois cette fonction dérivée sera utilisée dans la règle de mise à jour des poids par l'algorithme \n",
    "de rétropropagation du gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "\"\"\"\n",
    "Cette fonction recupere le resultat de la fonction mini_batch pour mettre à jours\n",
    "les valeurs du biais et du poids de chaque mini-batch. Cette fonction calcule le gradient pour chaque\n",
    "entrainement de mini-batch.\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "\"\"\"\"\n",
    "La fonction feedforward permet d'avoir une structure dans laquelle les neuronnes sont connectés \n",
    "que dans un sens : les features proviennent des entrées et vont traverser les couches cachés pour\n",
    "eventuellement atteindre les couches de sorties. Un réseau feedforward possède des neuronnes organisés\n",
    "dans des couches distinctes. La couche d'entrée introduit les variables d'entrée, et les neuronnes de la \n",
    "couche cachée et de la couche de sortie sont connéctés à leurs couches précédente.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "                \n",
    "\"\"\" Implémentation de la descente du gradient stochastique\n",
    "Dans ce code, la base d'entrainement est echantillonnée aléatoirement dans des partitions \n",
    "que l'on appelle mini-batch. Pour chacun des ces mini-batch on applique la descente du gradient\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./neural-networks-and-deep-learning-master/src/\")\n",
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9015 / 10000\n",
      "Epoch 1: 9184 / 10000\n",
      "Epoch 2: 9283 / 10000\n",
      "Epoch 3: 9345 / 10000\n",
      "Epoch 4: 9395 / 10000\n",
      "Epoch 5: 9394 / 10000\n",
      "Epoch 6: 9385 / 10000\n",
      "Epoch 7: 9386 / 10000\n",
      "Epoch 8: 9448 / 10000\n",
      "Epoch 9: 9451 / 10000\n",
      "Epoch 10: 9431 / 10000\n",
      "Epoch 11: 9449 / 10000\n",
      "Epoch 12: 9478 / 10000\n",
      "Epoch 13: 9478 / 10000\n",
      "Epoch 14: 9470 / 10000\n",
      "Epoch 15: 9462 / 10000\n",
      "Epoch 16: 9477 / 10000\n",
      "Epoch 17: 9487 / 10000\n",
      "Epoch 18: 9484 / 10000\n",
      "Epoch 19: 9496 / 10000\n",
      "Epoch 20: 9501 / 10000\n",
      "Epoch 21: 9510 / 10000\n",
      "Epoch 22: 9489 / 10000\n",
      "Epoch 23: 9492 / 10000\n",
      "Epoch 24: 9489 / 10000\n",
      "Epoch 25: 9519 / 10000\n",
      "Epoch 26: 9497 / 10000\n",
      "Epoch 27: 9501 / 10000\n",
      "Epoch 28: 9536 / 10000\n",
      "Epoch 29: 9534 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Avec cette exemple on peut observer que le reseau nous donne pour le 28 eme epoche un resultat de 95,02 %'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test sur exemple simple\n",
    "\"\"\"\n",
    "import network\n",
    "net = network.Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
    "\n",
    "\"\"\"\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
    "30 = nombres epochs\n",
    "10 taille des mini-batch\n",
    "et le learning rate\n",
    "\"\"\"\n",
    "\n",
    "\"Avec cette exemple on peut observer que le reseau nous donne pour le 28 eme epoche un resultat de 95,02 %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1135 / 10000\n",
      "Epoch 1: 1010 / 10000\n",
      "Epoch 2: 1135 / 10000\n",
      "Epoch 3: 1009 / 10000\n",
      "Epoch 4: 1032 / 10000\n",
      "Epoch 5: 1135 / 10000\n",
      "Epoch 6: 1010 / 10000\n",
      "Epoch 7: 1135 / 10000\n",
      "Epoch 8: 1135 / 10000\n",
      "Epoch 9: 1028 / 10000\n",
      "Epoch 10: 1135 / 10000\n",
      "Epoch 11: 1028 / 10000\n",
      "Epoch 12: 1028 / 10000\n",
      "Epoch 13: 1010 / 10000\n",
      "Epoch 14: 1028 / 10000\n",
      "Epoch 15: 1135 / 10000\n",
      "Epoch 16: 1010 / 10000\n",
      "Epoch 17: 982 / 10000\n",
      "Epoch 18: 1028 / 10000\n",
      "Epoch 19: 1135 / 10000\n",
      "Epoch 20: 958 / 10000\n",
      "Epoch 21: 1032 / 10000\n",
      "Epoch 22: 958 / 10000\n",
      "Epoch 23: 892 / 10000\n",
      "Epoch 24: 1135 / 10000\n",
      "Epoch 25: 1135 / 10000\n",
      "Epoch 26: 892 / 10000\n",
      "Epoch 27: 1135 / 10000\n",
      "Epoch 28: 1009 / 10000\n",
      "Epoch 29: 1135 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nNous pouvons observer ici que sans les neuronnes cach\\xc3\\xa9, le resultat de chaque epoch et mieux que le test precedent.\\n\\nCependant l'avantage des couches cach\\xc3\\xa9e est que la retropropagation marche pour n'importe quel nombre de couches cach\\xc3\\xa9, de plus \\navec les couches cach\\xc3\\xa9 la representation est modif\\xc3\\xa9e.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Nous allons maintenant faire le test avec 0 neuronnes caché et comparer avec le test precedent\"\"\"\n",
    "net1 = network.Network([784,0,10])\n",
    "net1.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
    "\n",
    "\"\"\"\n",
    "Nous pouvons observer ici que sans les neuronnes caché, le resultat de chaque epoch est mauvais et que les valeurs\n",
    "prises lors de chaque descente de gradient sont moins stable que dans le cas ou l'on une couche cachée\n",
    "L'avantage des couches cachée est que la retropropagation marche pour n'importe quel nombre de couches caché, de plus \n",
    "avec les couches caché la representation est modifée.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "On peut clairement observer ici que l'on a une performance moyenne de 11 % alors qu'avec des couches\n",
    "cachée on est à 95 %...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9518 / 10000\n",
      "Epoch 1: 9534 / 10000\n",
      "Epoch 2: 9533 / 10000\n",
      "Epoch 3: 9519 / 10000\n",
      "Epoch 4: 9523 / 10000\n",
      "Epoch 5: 9522 / 10000\n",
      "Epoch 6: 9518 / 10000\n",
      "Epoch 7: 9518 / 10000\n",
      "Epoch 8: 9526 / 10000\n",
      "Epoch 9: 9523 / 10000\n",
      "Epoch 10: 9512 / 10000\n",
      "Epoch 11: 9518 / 10000\n",
      "Epoch 12: 9522 / 10000\n",
      "Epoch 13: 9516 / 10000\n",
      "Epoch 14: 9516 / 10000\n",
      "Epoch 15: 9513 / 10000\n",
      "Epoch 16: 9521 / 10000\n",
      "Epoch 17: 9525 / 10000\n",
      "Epoch 18: 9518 / 10000\n",
      "Epoch 19: 9520 / 10000\n",
      "Epoch 20: 9518 / 10000\n",
      "Epoch 21: 9522 / 10000\n",
      "Epoch 22: 9529 / 10000\n",
      "Epoch 23: 9514 / 10000\n",
      "Epoch 24: 9505 / 10000\n",
      "Epoch 25: 9524 / 10000\n",
      "Epoch 26: 9521 / 10000\n",
      "Epoch 27: 9524 / 10000\n",
      "Epoch 28: 9515 / 10000\n",
      "Epoch 29: 9529 / 10000\n",
      "Epoch 30: 9523 / 10000\n",
      "Epoch 31: 9512 / 10000\n",
      "Epoch 32: 9511 / 10000\n",
      "Epoch 33: 9522 / 10000\n",
      "Epoch 34: 9519 / 10000\n",
      "Epoch 35: 9523 / 10000\n",
      "Epoch 36: 9521 / 10000\n",
      "Epoch 37: 9520 / 10000\n",
      "Epoch 38: 9520 / 10000\n",
      "Epoch 39: 9523 / 10000\n",
      "Epoch 40: 9516 / 10000\n",
      "Epoch 41: 9519 / 10000\n",
      "Epoch 42: 9513 / 10000\n",
      "Epoch 43: 9515 / 10000\n",
      "Epoch 44: 9520 / 10000\n",
      "Epoch 45: 9523 / 10000\n",
      "Epoch 46: 9514 / 10000\n",
      "Epoch 47: 9516 / 10000\n",
      "Epoch 48: 9515 / 10000\n",
      "Epoch 49: 9513 / 10000\n",
      "Epoch 50: 9520 / 10000\n",
      "Epoch 51: 9518 / 10000\n",
      "Epoch 52: 9512 / 10000\n",
      "Epoch 53: 9514 / 10000\n",
      "Epoch 54: 9519 / 10000\n",
      "Epoch 55: 9518 / 10000\n",
      "Epoch 56: 9519 / 10000\n",
      "Epoch 57: 9514 / 10000\n",
      "Epoch 58: 9520 / 10000\n",
      "Epoch 59: 9512 / 10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Nous allons maintenant augmenter le nombre d'epochs et d'echantillon mini-batch \"\"\" \n",
    "\n",
    "net2= network.Network([784,30,10])\n",
    "net2.SGD(training_data, 60, 20, 3.0, test_data=test_data)\n",
    "\n",
    "\"\"\"Avec plus d'echantillons mini-batch et un plus grand nombre de calcul du gradient\n",
    "Dans ce test nous avons une moyenne de 95 % de performance, ce qui est quasi-similaire à notre premier test\n",
    "nous pouvons faire la conjecture l'augmentation du nombre d'epoch et les mini-batch permet d'ameliorer\n",
    "la precision de notre systeme \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8697 / 10000\n",
      "Epoch 1: 8955 / 10000\n",
      "Epoch 2: 9090 / 10000\n",
      "Epoch 3: 9162 / 10000\n",
      "Epoch 4: 9221 / 10000\n",
      "Epoch 5: 9262 / 10000\n",
      "Epoch 6: 9273 / 10000\n",
      "Epoch 7: 9308 / 10000\n",
      "Epoch 8: 9302 / 10000\n",
      "Epoch 9: 9334 / 10000\n",
      "Epoch 10: 9306 / 10000\n",
      "Epoch 11: 9349 / 10000\n",
      "Epoch 12: 9369 / 10000\n",
      "Epoch 13: 9363 / 10000\n",
      "Epoch 14: 9352 / 10000\n",
      "Epoch 15: 9366 / 10000\n",
      "Epoch 16: 9357 / 10000\n",
      "Epoch 17: 9362 / 10000\n",
      "Epoch 18: 9379 / 10000\n",
      "Epoch 19: 9382 / 10000\n",
      "Epoch 20: 9388 / 10000\n",
      "Epoch 21: 9383 / 10000\n",
      "Epoch 22: 9405 / 10000\n",
      "Epoch 23: 9399 / 10000\n",
      "Epoch 24: 9402 / 10000\n",
      "Epoch 25: 9392 / 10000\n",
      "Epoch 26: 9416 / 10000\n",
      "Epoch 27: 9395 / 10000\n",
      "Epoch 28: 9399 / 10000\n",
      "Epoch 29: 9400 / 10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"Nous allons maintenant observer l'influence du learning rate en le reduisant  à 1.0 au lieu de 3.0 \"\"\"\n",
    "\n",
    "net3= network.Network([784,30,10])\n",
    "net3.SGD(training_data, 30, 10, 1.0, test_data=test_data)\n",
    "\"\"\"\n",
    "Nous pouvons observer une baisse sur de la performance \"\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6845 / 10000\n",
      "Epoch 1: 7866 / 10000\n",
      "Epoch 2: 8028 / 10000\n",
      "Epoch 3: 8105 / 10000\n",
      "Epoch 4: 8189 / 10000\n",
      "Epoch 5: 8231 / 10000\n",
      "Epoch 6: 8260 / 10000\n",
      "Epoch 7: 8273 / 10000\n",
      "Epoch 8: 8320 / 10000\n",
      "Epoch 9: 8315 / 10000\n",
      "Epoch 10: 8361 / 10000\n",
      "Epoch 11: 8368 / 10000\n",
      "Epoch 12: 8393 / 10000\n",
      "Epoch 13: 8396 / 10000\n",
      "Epoch 14: 8383 / 10000\n",
      "Epoch 15: 8395 / 10000\n",
      "Epoch 16: 8399 / 10000\n",
      "Epoch 17: 8419 / 10000\n",
      "Epoch 18: 8412 / 10000\n",
      "Epoch 19: 8430 / 10000\n",
      "Epoch 20: 8427 / 10000\n",
      "Epoch 21: 8432 / 10000\n",
      "Epoch 22: 8433 / 10000\n",
      "Epoch 23: 8442 / 10000\n",
      "Epoch 24: 8437 / 10000\n",
      "Epoch 25: 8445 / 10000\n",
      "Epoch 26: 8451 / 10000\n",
      "Epoch 27: 8459 / 10000\n",
      "Epoch 28: 8459 / 10000\n",
      "Epoch 29: 8464 / 10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test avec learning rate 0.5\"\"\"\n",
    "net4= network.Network([784,30,10])\n",
    "net4.SGD(training_data, 30, 10, 0.5, test_data=test_data)\n",
    "\n",
    "\"\"\"Les resultats sont sont moins bon qu'un seuil de 3.0. On peut faire la conjecture\n",
    "que plus le learning rate est faible et moins bon sont les resultats.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
